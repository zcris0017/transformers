# transformers_introduction
This is a series of files that included comments to the code in a blog from Harvard NLP, The Annotated Transformer, which is included on (http://nlp.seas.harvard.edu/annotated-transformer/).
The original paper, Attention Is All You Need, is available on (https://arxiv.org/abs/1706.03762).

The comments could be stated in one line will be annoted with #, the comments using more than one line is annoted between \" or \""". To distinguish original comments and ones I wrote on my own, the comments I added did not use capital letter at the beginning of the comments.

The file transformer.py focused on the transformers structure. I will soon update the Complete_Model.py, which describes how we sett up and train the whole model described in the blog, by the end of week 2, September.

Please put both files in the same folder if you want to run the Complete_Model.py.

Please feel free to email me at zcr1281474170@163.com if there is anything wrong with the explanation to the codes.
