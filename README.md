# transformers_introduction
This is a file that included comments to the code in a blog from Harvard NLP, The Annotated Transformer, which is included on (http://nlp.seas.harvard.edu/annotated-transformer/).
The original paper, Attention Is All You Need, is available on (https://arxiv.org/abs/1706.03762).

The comments could be stated in one line will be annoted with #, the comments using more than one line is annoted between \" or \""". To distinguish original comments and ones I wrote on my own, the comments I added did not use capital letter at the beginning of the comments.

Please feel free to email me at zcr1281474170@163.com if there is anything wrong with the explanation to the codes.
